{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE REINFRORCEMENT LEARNING PROBLEM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.1: \n",
    "Self-Play Suppose, instead of playing against a random\n",
    "opponent, the reinforcement learning algorithm described above played against\n",
    "itself. What do you think would happen in this case? Would it learn a different\n",
    "way of playing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANSWER\n",
    "It would learn a different policy than the earlier fixed opponent, since now the agent(opponent) it is interacting with is changing too. It may not reach an optimal policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.2:\n",
    "Symmetries Many tic-tac-toe positions appear di\u000b",
    "erent but\n",
    "are really the same because of symmetries. How might we amend the reinforcement\n",
    "learning algorithm described above to take advantage of this? In what\n",
    "ways would this improve it? Now think again. Suppose the opponent did not\n",
    "take advantage of symmetries. In that case, should we? Is it true, then, that\n",
    "symmetrically equivalent positions should necessarily have the same value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANSWER\n",
    "Here , the opponent is the part of the environment which keeps on changing , so marking states as same based on symmetries is a bad idea. Also, in order to decrease the search space, unique states can be marked until the symmetry is reached. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.3: \n",
    "Greedy Play Suppose the reinforcement learning player was\n",
    "greedy, that is, it always played the move that brought it to the position that\n",
    "it rated the best. Would it learn to play better, or worse, than a nongreedy\n",
    "player? What problems might occur?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer \n",
    "It will learn to play worse as the greedy player is not exploring each state for possible better rewards.Basically , no exploration occurs so performance is not close to optimum.\n",
    "This problem adds up to error in judgements at a later time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.4: \n",
    "Learning from Exploration Suppose learning updates occurred\n",
    "after all moves, including exploratory moves. If the step-size parameter is\n",
    "appropriately reduced over time, then the state values would converge to a\n",
    "set of probabilities. What are the two sets of probabilities computed when we\n",
    "do, and when we do not, learn from exploratory moves? Assuming that we\n",
    "do continue to make exploratory moves, which set of probabilities might be\n",
    "better to learn? Which would result in more wins?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer \n",
    "We can assign higher probabilities to the actions which are optimal (optimal moves). Updating the optimal move further will increase the chances of the agent winning or cause more wins. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.5:\n",
    "Other Improvements Can you think of other ways to improve\n",
    "the reinforcement learning player? Can you think of any better way to solve\n",
    "the tic-tac-toe problem as posed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "Maybe use an Image of the gameboard at each timestep as an observation(instantiation of the state space) producing a particular action.Basically learning the policy. Making better reward signals , like ranking Draws higher than losses.  Using Random searching techniques to search for a better policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
