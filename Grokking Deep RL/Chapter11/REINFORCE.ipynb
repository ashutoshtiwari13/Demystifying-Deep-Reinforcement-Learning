{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "REINFORCE",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_iNCBbwmZ7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Google colab file \n",
        "#Created by ashutoshtiwari13\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.multiprocessing as mp\n",
        "import threading \n",
        "\n",
        "import numpy as np \n",
        "from collections import namedtuple, deque\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pylab as pylab\n",
        "from itertools import cycle, count "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lskWhAe2OaDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib\n",
        "import gym\n",
        "import os\n",
        "import io\n",
        "import sys\n",
        "import time \n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftNjxdX8OvXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CartPoleEnvV1(gym.Wrapper):\n",
        "  def __init__(self,env):\n",
        "    gym.Wrapper.__init__(self,env)\n",
        "  def reset(self, **kwargs):\n",
        "    return self.env.reset(**kwargs)\n",
        "  def step(self, action):\n",
        "    next_state, reward,done, info = self.env.step(action)\n",
        "    (x, x_dot, theta, theta_dot) = next_state\n",
        "    pole_fall = x < -self.env.unwrapped.x_threshold or x > self.env.unwrapped.x_threshold or theta < -self.env.unwrapped.theta_threshold_radians or theta >  self.env.unwrapped.theta_threshold_radians\n",
        "    reward = -1 if pole_fell else 0\n",
        "    return next_state, reward, done, info"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtEFNoCHStBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CartPoleEnvV2(gym.Wrapper):\n",
        "  def __init__(self,env):\n",
        "    gym.Wrapper.__init__(self,env)\n",
        "  def reset(self, **kwargs):\n",
        "    return self.env.reset(**kwargs)\n",
        "  def step(self, action):\n",
        "    next_state, reward,done, info = self.env.step(action)\n",
        "    (x, x_dot, theta, theta_dot) = next_state\n",
        "    pole_fall = x < -self.env.unwrapped.x_threshold or x > self.env.unwrapped.x_threshold or theta < -self.env.unwrapped.theta_threshold_radians or theta >  self.env.unwrapped.theta_threshold_radians\n",
        "    \n",
        "    if done:\n",
        "      if pole_fell:\n",
        "        reward=0\n",
        "      else:\n",
        "        reward = self.env._max_episode_steps\n",
        "    return next_state, reward, done, info"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caLuk4elSh9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NN_arch(nn.Module):\n",
        "  def __init__(Self, input_dim, output_dim, hidden_dim=(32,32),activation = F.relu):\n",
        "    super(NN_arch, self).__init__()\n",
        "    self.activation = activation\n",
        "    self.input = nn.Linear(input_dim , hidden_dims[0])\n",
        "    self.hidden_layers = nn.ModuleList()\n",
        "    for i in range(len(hidden_dims)-1):\n",
        "      hidden = nn.Linear(hidden_dims[i],hidden_dims[i+1])\n",
        "      self.hidden.append(hidden)\n",
        "    self.output = nn.Linear(hidden_dims[-1],output_dim)\n",
        "\n",
        "  def stateChange(self, state):\n",
        "    \"\"\"\n",
        "    Making sure the state is the type of variable and shape before passing to the NN_arch\n",
        "    \"\"\"\n",
        "    x = state\n",
        "    if not isinstance(x, torch.Tensor):\n",
        "      x = torch.tensor(x, dtype= torch.float32)\n",
        "      x = x.unsqueeze(0)\n",
        "\n",
        "    x = self.activation(self.input(x))\n",
        "    for h in hidden:\n",
        "      x = self.activation(hidden)\n",
        "    return self.output(x)\n",
        "  \n",
        "  def forwardPassAction(self, state):\n",
        "    #stateChange returns logits, preferences over actions\n",
        "    logits = self.stateChange(state)\n",
        "    #sample the action from the probability distribution\n",
        "    dist = torch.distributions.Categorical(logits=logits)\n",
        "    action = dist.sample()\n",
        "    #calculate the log probablity of that action and format it for training \n",
        "    logpa = dist.log_prob(action).unsqueeze(-1)\n",
        "    #determine the entropy of the policy \n",
        "    entropy = dist.entropy().unsqueeze(-1)\n",
        "    #Keeping a check on whether the policy selected was exploratory or not\n",
        "    is_exploratory = action != np.argmax(logits.detach().numpy())\n",
        "    \"\"\"\n",
        "    returns:\n",
        "    The action that can directly pe passed to the env\n",
        "    Flag to check whether the action was exploratory\n",
        "    log probability of the action \n",
        "    entropy of the policy \n",
        "    \"\"\"\n",
        "    return action.item(), is_exploratory.item(),logpa,entropy\n",
        "\n",
        "  def actionSample(self, action):\n",
        "    logits = self.stateChange(state)\n",
        "    dist = torch.distributions.Categorical(logits=logits)\n",
        "    action = dist.sample()\n",
        "    return action.item()\n",
        "\n",
        "  def greedyAction(self, state):\n",
        "    #select the greedy action according to the policy \n",
        "    logits = self.stateChange(state)\n",
        "    return np.argmax(logits.detach().numpy())    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7U7DeQo0-KC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class REINFORCE():\n",
        "  def __init__(self, policy_model_fn , policy_optimizer_fn, policy_optimizer_lr):\n",
        "    self.policy_model_fn =policy_model_fn\n",
        "    self.policy_optimizer_fn =policy_optimizer_fn\n",
        "    self.policy_optimizer_fn = policy_optimizer_fn\n",
        "\n",
        "  def optimize_model(self):\n",
        "    T = len(self.rewards)\n",
        "    #logspace helps in setting up gamma values ans returns a series of timestep[ 1, 0.99, 0.9801]\n",
        "    discounts = np.logspace(0,T ,num=T, base =self.gamma, endpoint =False)\n",
        "    #sum of discounted returns fro all timesteps\n",
        "    returns = np.array([np.sum(discounts[:T-t] * self.rewards[t:]) for t in range(T)])\n",
        "\n",
        "    \"\"\"\n",
        "    Policy loss :\n",
        "    Log probability of the actions selected weighted by the returns obtained after that action\n",
        "    was selected. We need to minimize the loss, so using negative mean. Also, to account fro\n",
        "    discounted policy gradients, we multiply the returns by the discounts\n",
        "    \"\"\"\n",
        "\n",
        "    discounts = torch.FloatTensor(discounts).unsqueeze(1)\n",
        "    returns = torch.FloatTensor(returns).unsqueeze(1)\n",
        "    self.logpas = torch.cat(self.logpas)\n",
        "\n",
        "    policy_loss = -(discounts * returns * self.logpas).mean()\n",
        "\n",
        "    #Step1 : zero the gradient in the optimizer\n",
        "    #Step2 : back propagation\n",
        "    #Step3 : step in the direction of the gradient\n",
        "    self.policy_optimizer.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    self.policy_optimizer.step()\n",
        "\n",
        "  def interaction_step(self,state, env):\n",
        "      \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}