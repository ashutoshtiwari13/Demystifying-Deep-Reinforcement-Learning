{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MonteCarloMethods.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "caOzjcHx9WeA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import gym \n",
        "import numpy as np\n",
        "from collections import defaultdict \n",
        "from utils import plot_blackjack_values,plot_policy\n",
        "\n",
        "env= gym.make('Blackjack-v0')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnPGkzNYCU0p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f7b96691-9d1d-4db8-cda2-4616c789e543"
      },
      "source": [
        "print(env.observation_space)\n",
        "print(env.action_space)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
            "Discrete(2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOCwuTOJCg0L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "b270ff4b-13e0-4a74-e1dc-047fb4a4a2b7"
      },
      "source": [
        "### Random Policy \n",
        "for episodes in range(3):\n",
        "  state = env.reset()\n",
        "  while True:\n",
        "    print(state)\n",
        "    action = env.action_space.sample() \n",
        "    state, reward , done, info = env.step(action)\n",
        "    if done:\n",
        "      print(\"End Game : Reward -\", reward)\n",
        "      print(\"You won ! Hola :D\") if reward > 0 else print(\"You lost ! :(\")\n",
        "      break"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15, 5, False)\n",
            "End Game : Reward - 1.0\n",
            "You won ! Hola :D\n",
            "(12, 3, False)\n",
            "End Game : Reward - 1.0\n",
            "You won ! Hola :D\n",
            "(10, 10, False)\n",
            "End Game : Reward - -1.0\n",
            "You lost ! :(\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClaYXdIgxRV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Monte-Carlo Prediction (For the present env , First visit and every visit result in the same solution)\n",
        "##Stochastic episode generation  \n",
        "\n",
        "def generate_episode(env_instance):\n",
        "  episode =[]\n",
        "  state= env.reset()\n",
        "  while True:\n",
        "    probs  = [0.8, 0.2] if state[0] > 18 else [0.2,0.8]\n",
        "    action = np.random.choice(np.arange(2),p =probs)\n",
        "    next_state , reward,done , info = env.step(action)\n",
        "    episode.append((state, action , reward))\n",
        "    state = next_state\n",
        "    if done:\n",
        "      break\n",
        "  \n",
        "  return episode"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJRLbGOgxThr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Monte-carlo every visit Control \n",
        "\n",
        "def every_visit_monte_carlo_control(env , num_episodes , generate_episode, gamma =1.0):\n",
        "  total_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "  iters = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "  Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "\n",
        "  for episodes in range(1, num_episodes+1):\n",
        "    if episodes %10000 ==0:\n",
        "      print(\"\\rEpisode {}/{}.\".format(episodes,num_episodes),end = \"\")\n",
        "    episode = generate_episode(env)\n",
        "    states , actions, rewards = zip(*episode)\n",
        "  \n",
        "    #consideing the case of discounting \n",
        "    discounts = np.array([gamma ** i for i in range(len(rewards)+1)])\n",
        "\n",
        "    for i , state in enumerate(states):\n",
        "\n",
        "      total_sum[state][actions[i]] += sum(rewards[i:]* discounts[:-(1+i)])\n",
        "      iters[states][actions[i]] += 1.0\n",
        "      Q[states][actions[i]] += total_sum[state][actions[i]]/iters[states][actions[i]]\n",
        "  return Q"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOwuPbuzxV-0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9fcba485-8013-4697-a322-5bf80c1a196e"
      },
      "source": [
        "#result\n",
        "Q = every_visit_monte_carlo_control(env, 500000,generate_episode)\n",
        "# for k, v in Q.items():\n",
        "#   print(k)\n",
        "#   print(v)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 500000/500000."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8pwsdjxx6wK",
        "colab_type": "text"
      },
      "source": [
        "**Constant Alpha Monte-carlo Control and GLIE (Greedy in the limit with infinite Exploration)** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOXvcQ84yHnL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_episode_for_glie(env, Q, epsilon , nA):\n",
        "  \"\"\"generates a state for the epsilon-greedy policy\"\"\"\n",
        "  episode_glie =[]\n",
        "  state = env.reset()\n",
        "  while True:\n",
        "    # action = env.action_space.sample()\n",
        "    action = np.random.choice(np.arange(nA), p = get_probs(Q[state],epsilon,nA)) if state in Q else env.action_space.sample()\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    episode_glie.append((state, action, reward))\n",
        "    state = next_state\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "  return episode_glie    \n",
        "\n",
        "def get_probs(Q_s , epsilon , nA):\n",
        "  policy = np.ones(nA) * epsilon/nA\n",
        "  best_action =  np.argmax(Q_s)\n",
        "  policy[best_action] = 1- epsilon + (epsilon/nA)\n",
        "  return policy "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJyNXA8lyETc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_step_q(env,episode,Q,alpha, gamma):\n",
        "  #updating Q step \n",
        "  states , actions , rewards  = zip(*episode)\n",
        "  discounts = np.array([gamma ** i for i in range(len(rewards)+1)])\n",
        "  for i , state in enumerate(states):\n",
        "    prior_Q = Q[states][actions[i]]\n",
        "    Q[states][actions[i]] = prior_Q + alpha*(sum(rewards[i:]*discounts[:-(1+i)]) - prior_Q)\n",
        "\n",
        "  return Q  \n",
        "\n",
        "\n",
        "\n",
        "def constant_alpha_monte_carlo_control(env, num_episodes, alpha, gamma =1.0, eps_start=1.0, eps_decay =0.99999,eps_min=0.05):\n",
        "  nA = env.action_space.n\n",
        "  Q = defaultdict(lambda: np.zeros(nA))\n",
        "  epsilon = eps_start\n",
        "\n",
        "  for episodes in range(1, num_episodes+1):\n",
        "\n",
        "    if episodes%10000==0:\n",
        "      print(\"\\rEpisode {}/{}.\".format(episodes,num_episodes),end = \"\")\n",
        "    epsilon = max(epsilon*eps_decay,eps_min)\n",
        "    episode = generate_episode_for_glie(env, Q, epsilon, nA)\n",
        "    Q = update_step_q(env, episode, Q, alpha, gamma)\n",
        "\n",
        "  policy = dict((k,np.argmax(v)) for k,v in Q.items())\n",
        "  return policy , Q    \n",
        "             "
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-xkTH895WR6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "59684918-789a-4b33-f632-f5363dfabaa8"
      },
      "source": [
        "policy, Q = constant_alpha_monte_carlo_control(env, 500000,0.02)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 500000/500000."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s5kaY_w4dUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot = dict((k,np.max(v)) for k, v in Q.items())\n",
        "# plot_blackjack_values(plot)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ods5lKae5yCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot_policy(policy)"
      ],
      "execution_count": 43,
      "outputs": []
    }
  ]
}