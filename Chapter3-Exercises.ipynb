{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.1 \n",
    "Devise three example tasks of your own that fit into the reinforcement\n",
    "learning framework, identifying for each its states, actions, and\n",
    "rewards. Make the three examples as different from each other as possible.\n",
    "The framework is abstract and \n",
    "exible and can be applied in many different\n",
    "ways. Stretch its limits in some way in at least one of your examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "1. Training my dog to sit \n",
    "states : Dog's attention , Dog getting tired, Dog is goving away , Dog obeying commands \n",
    "actions : Talking to the dog , Showing treat , Moving hand, Give treat\n",
    "Reward : the treat value\n",
    "\n",
    "2. Car driving :\n",
    "states : destination approach , traffic jam in the street , starting to rain \n",
    "actions : Steer by X , accelerate by X , Break by X , Velocity \n",
    "reward : driving the car safely \n",
    "\n",
    "3. Football coach \n",
    "states :  current score , agility of players \n",
    "actions : dribble , tackle , shoot , other strategies \n",
    "reward : Scoring a goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.2 \n",
    "Is the reinforcement learning framework adequate to usefully\n",
    "represent all goal-directed learning tasks? Can you think of any clear exceptions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer \n",
    "The tasks that don't follow the markov property are difficult to solve using reinforcement learning one-step dynamics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.3 \n",
    "Consider the problem of driving. You could define the actions\n",
    "in terms of the accelerator, steering wheel, and brake, that is, where your\n",
    "body meets the machine. Or you could define them farther out|say, where\n",
    "the rubber meets the road, considering your actions to be tire torques. Or\n",
    "you could define them farther in|say, where your brain meets your body, the\n",
    "actions being muscle twitches to control your limbs. Or you could go to a\n",
    "really high level and say that your actions are your choices of where to drive.\n",
    "What is the right level, the right place to draw the line between agent and\n",
    "environment? On what basis is one location of the line to be preferred over\n",
    "another? Is there any fundamental reason for preferring one location over\n",
    "another, or is it a free choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "The line between agent and the environment depends on the task. It is the task setting whcih defines what type of actions, observations(instantiation of states) , goals need to be established.So it if the task is to cross the traffic light, brakes , acceleration etc become important.\n",
    "\n",
    "Also,I think the basis of the location of this distinction also depends on the decision maker-- in the example above it's the human brain.\n",
    "It is important to chose this line properly as the decision maker should able to take control of the actions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.4 \n",
    "Suppose you treated pole-balancing as an episodic task but\n",
    "also used discounting, with all rewards zero except for -1 upon failure. What\n",
    "then would the return be at each time? How does this return differ from that\n",
    "in the discounted, continuing formulation of this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "The return in both the cases will be negative and further will become less negative indicating that the agent is learning. Aslo, in both cases the agent ends with falling with a probablity of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.5 \n",
    "Imagine that you are designing a robot to run a maze. You decide\n",
    "to give it a reward of +1 for escaping from the maze and a reward of zero\n",
    "at all other times. The task seems to break down naturally into episodes|the\n",
    "successive runs through the maze|so you decide to treat it as an episodic task,\n",
    "where the goal is to maximize expected total reward (3.1). After running the\n",
    "learning agent for a while, you find that it is showing no improvement in escaping\n",
    "from the maze. What is going wrong? Have you effectively communicated\n",
    "to the agent what you want it to achieve?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "Maybe the agent is stuck in a loop , because of which we might need to change the strategy we are using to guide the robot out of the maze as quickly as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.6: \n",
    "Broken Vision System Imagine that you are a vision\n",
    "system. When you are first turned on for the day, an image \n",
    "floods into your camera. You can see lots of things, but not all things. You can't see objects\n",
    "that are occluded, and of course you can't see objects that are behind you.\n",
    "After seeing that first scene, do you have access to the Markov state of the\n",
    "environment? Suppose your camera was broken that day and you received no images at all, all day. Would you have access to the Markov state then?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer.\n",
    "Yes we do have the Markov State  of the environment. We don't need to know everything about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
